{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.099, Accuracy: 0.333\n",
      "Epoch 100, Loss: 1.098, Accuracy: 0.370\n",
      "Epoch 200, Loss: 1.098, Accuracy: 0.437\n",
      "Epoch 300, Loss: 1.098, Accuracy: 0.423\n",
      "Epoch 400, Loss: 1.098, Accuracy: 0.420\n",
      "Epoch 500, Loss: 1.097, Accuracy: 0.420\n",
      "Epoch 600, Loss: 1.096, Accuracy: 0.407\n",
      "Epoch 700, Loss: 1.095, Accuracy: 0.410\n",
      "Epoch 800, Loss: 1.093, Accuracy: 0.400\n",
      "Epoch 900, Loss: 1.091, Accuracy: 0.403\n",
      "Epoch 1000, Loss: 1.088, Accuracy: 0.397\n",
      "Epoch 1100, Loss: 1.086, Accuracy: 0.393\n",
      "Epoch 1200, Loss: 1.084, Accuracy: 0.400\n",
      "Epoch 1300, Loss: 1.082, Accuracy: 0.400\n",
      "Epoch 1400, Loss: 1.081, Accuracy: 0.407\n",
      "Epoch 1500, Loss: 1.081, Accuracy: 0.413\n",
      "Epoch 1600, Loss: 1.080, Accuracy: 0.417\n",
      "Epoch 1700, Loss: 1.079, Accuracy: 0.430\n",
      "Epoch 1800, Loss: 1.079, Accuracy: 0.433\n",
      "Epoch 1900, Loss: 1.078, Accuracy: 0.430\n",
      "Epoch 2000, Loss: 1.078, Accuracy: 0.433\n",
      "Epoch 2100, Loss: 1.078, Accuracy: 0.440\n",
      "Epoch 2200, Loss: 1.078, Accuracy: 0.437\n",
      "Epoch 2300, Loss: 1.078, Accuracy: 0.437\n",
      "Epoch 2400, Loss: 1.077, Accuracy: 0.433\n",
      "Epoch 2500, Loss: 1.077, Accuracy: 0.433\n",
      "Epoch 2600, Loss: 1.077, Accuracy: 0.433\n",
      "Epoch 2700, Loss: 1.077, Accuracy: 0.430\n",
      "Epoch 2800, Loss: 1.077, Accuracy: 0.430\n",
      "Epoch 2900, Loss: 1.077, Accuracy: 0.430\n",
      "Epoch 3000, Loss: 1.077, Accuracy: 0.430\n",
      "Epoch 3100, Loss: 1.076, Accuracy: 0.430\n",
      "Epoch 3200, Loss: 1.076, Accuracy: 0.433\n",
      "Epoch 3300, Loss: 1.076, Accuracy: 0.437\n",
      "Epoch 3400, Loss: 1.076, Accuracy: 0.440\n",
      "Epoch 3500, Loss: 1.076, Accuracy: 0.440\n",
      "Epoch 3600, Loss: 1.076, Accuracy: 0.440\n",
      "Epoch 3700, Loss: 1.075, Accuracy: 0.440\n",
      "Epoch 3800, Loss: 1.075, Accuracy: 0.443\n",
      "Epoch 3900, Loss: 1.075, Accuracy: 0.443\n",
      "Epoch 4000, Loss: 1.075, Accuracy: 0.443\n",
      "Epoch 4100, Loss: 1.074, Accuracy: 0.440\n",
      "Epoch 4200, Loss: 1.074, Accuracy: 0.440\n",
      "Epoch 4300, Loss: 1.074, Accuracy: 0.433\n",
      "Epoch 4400, Loss: 1.073, Accuracy: 0.433\n",
      "Epoch 4500, Loss: 1.073, Accuracy: 0.430\n",
      "Epoch 4600, Loss: 1.072, Accuracy: 0.430\n",
      "Epoch 4700, Loss: 1.072, Accuracy: 0.427\n",
      "Epoch 4800, Loss: 1.071, Accuracy: 0.423\n",
      "Epoch 4900, Loss: 1.071, Accuracy: 0.427\n",
      "Epoch 5000, Loss: 1.070, Accuracy: 0.430\n",
      "Epoch 5100, Loss: 1.069, Accuracy: 0.433\n",
      "Epoch 5200, Loss: 1.069, Accuracy: 0.437\n",
      "Epoch 5300, Loss: 1.068, Accuracy: 0.430\n",
      "Epoch 5400, Loss: 1.067, Accuracy: 0.437\n",
      "Epoch 5500, Loss: 1.066, Accuracy: 0.440\n",
      "Epoch 5600, Loss: 1.065, Accuracy: 0.443\n",
      "Epoch 5700, Loss: 1.065, Accuracy: 0.450\n",
      "Epoch 5800, Loss: 1.064, Accuracy: 0.450\n",
      "Epoch 5900, Loss: 1.063, Accuracy: 0.447\n",
      "Epoch 6000, Loss: 1.062, Accuracy: 0.447\n",
      "Epoch 6100, Loss: 1.061, Accuracy: 0.447\n",
      "Epoch 6200, Loss: 1.060, Accuracy: 0.453\n",
      "Epoch 6300, Loss: 1.060, Accuracy: 0.447\n",
      "Epoch 6400, Loss: 1.059, Accuracy: 0.440\n",
      "Epoch 6500, Loss: 1.058, Accuracy: 0.443\n",
      "Epoch 6600, Loss: 1.057, Accuracy: 0.447\n",
      "Epoch 6700, Loss: 1.056, Accuracy: 0.443\n",
      "Epoch 6800, Loss: 1.056, Accuracy: 0.447\n",
      "Epoch 6900, Loss: 1.055, Accuracy: 0.440\n",
      "Epoch 7000, Loss: 1.054, Accuracy: 0.443\n",
      "Epoch 7100, Loss: 1.053, Accuracy: 0.437\n",
      "Epoch 7200, Loss: 1.052, Accuracy: 0.440\n",
      "Epoch 7300, Loss: 1.051, Accuracy: 0.440\n",
      "Epoch 7400, Loss: 1.050, Accuracy: 0.440\n",
      "Epoch 7500, Loss: 1.049, Accuracy: 0.453\n",
      "Epoch 7600, Loss: 1.048, Accuracy: 0.453\n",
      "Epoch 7700, Loss: 1.047, Accuracy: 0.453\n",
      "Epoch 7800, Loss: 1.046, Accuracy: 0.463\n",
      "Epoch 7900, Loss: 1.045, Accuracy: 0.457\n",
      "Epoch 8000, Loss: 1.044, Accuracy: 0.450\n",
      "Epoch 8100, Loss: 1.042, Accuracy: 0.453\n",
      "Epoch 8200, Loss: 1.041, Accuracy: 0.457\n",
      "Epoch 8300, Loss: 1.039, Accuracy: 0.457\n",
      "Epoch 8400, Loss: 1.038, Accuracy: 0.450\n",
      "Epoch 8500, Loss: 1.037, Accuracy: 0.450\n",
      "Epoch 8600, Loss: 1.035, Accuracy: 0.467\n",
      "Epoch 8700, Loss: 1.033, Accuracy: 0.463\n",
      "Epoch 8800, Loss: 1.032, Accuracy: 0.470\n",
      "Epoch 8900, Loss: 1.030, Accuracy: 0.487\n",
      "Epoch 9000, Loss: 1.029, Accuracy: 0.497\n",
      "Epoch 9100, Loss: 1.027, Accuracy: 0.507\n",
      "Epoch 9200, Loss: 1.025, Accuracy: 0.507\n",
      "Epoch 9300, Loss: 1.023, Accuracy: 0.513\n",
      "Epoch 9400, Loss: 1.021, Accuracy: 0.517\n",
      "Epoch 9500, Loss: 1.019, Accuracy: 0.513\n",
      "Epoch 9600, Loss: 1.016, Accuracy: 0.510\n",
      "Epoch 9700, Loss: 1.014, Accuracy: 0.507\n",
      "Epoch 9800, Loss: 1.012, Accuracy: 0.513\n",
      "Epoch 9900, Loss: 1.009, Accuracy: 0.513\n",
      "Epoch 10000, Loss: 1.007, Accuracy: 0.520\n",
      "Epoch 10100, Loss: 1.004, Accuracy: 0.527\n",
      "Epoch 10200, Loss: 1.001, Accuracy: 0.523\n",
      "Epoch 10300, Loss: 0.998, Accuracy: 0.523\n",
      "Epoch 10400, Loss: 0.995, Accuracy: 0.527\n",
      "Epoch 10500, Loss: 0.992, Accuracy: 0.527\n",
      "Epoch 10600, Loss: 0.989, Accuracy: 0.530\n",
      "Epoch 10700, Loss: 0.986, Accuracy: 0.533\n",
      "Epoch 10800, Loss: 0.983, Accuracy: 0.543\n",
      "Epoch 10900, Loss: 0.979, Accuracy: 0.547\n",
      "Epoch 11000, Loss: 0.975, Accuracy: 0.547\n",
      "Epoch 11100, Loss: 0.972, Accuracy: 0.550\n",
      "Epoch 11200, Loss: 0.968, Accuracy: 0.560\n",
      "Epoch 11300, Loss: 0.964, Accuracy: 0.570\n",
      "Epoch 11400, Loss: 0.960, Accuracy: 0.573\n",
      "Epoch 11500, Loss: 0.956, Accuracy: 0.580\n",
      "Epoch 11600, Loss: 0.952, Accuracy: 0.583\n",
      "Epoch 11700, Loss: 0.948, Accuracy: 0.587\n",
      "Epoch 11800, Loss: 0.944, Accuracy: 0.593\n",
      "Epoch 11900, Loss: 0.940, Accuracy: 0.590\n",
      "Epoch 12000, Loss: 0.936, Accuracy: 0.593\n",
      "Epoch 12100, Loss: 0.932, Accuracy: 0.587\n",
      "Epoch 12200, Loss: 0.927, Accuracy: 0.593\n",
      "Epoch 12300, Loss: 0.923, Accuracy: 0.597\n",
      "Epoch 12400, Loss: 0.919, Accuracy: 0.593\n",
      "Epoch 12500, Loss: 0.915, Accuracy: 0.597\n",
      "Epoch 12600, Loss: 0.910, Accuracy: 0.590\n",
      "Epoch 12700, Loss: 0.906, Accuracy: 0.590\n",
      "Epoch 12800, Loss: 0.902, Accuracy: 0.590\n",
      "Epoch 12900, Loss: 0.898, Accuracy: 0.600\n",
      "Epoch 13000, Loss: 0.893, Accuracy: 0.597\n",
      "Epoch 13100, Loss: 0.889, Accuracy: 0.597\n",
      "Epoch 13200, Loss: 0.885, Accuracy: 0.597\n",
      "Epoch 13300, Loss: 0.880, Accuracy: 0.603\n",
      "Epoch 13400, Loss: 0.876, Accuracy: 0.610\n",
      "Epoch 13500, Loss: 0.872, Accuracy: 0.610\n",
      "Epoch 13600, Loss: 0.868, Accuracy: 0.617\n",
      "Epoch 13700, Loss: 0.863, Accuracy: 0.623\n",
      "Epoch 13800, Loss: 0.859, Accuracy: 0.623\n",
      "Epoch 13900, Loss: 0.855, Accuracy: 0.623\n",
      "Epoch 14000, Loss: 0.851, Accuracy: 0.620\n",
      "Epoch 14100, Loss: 0.847, Accuracy: 0.620\n",
      "Epoch 14200, Loss: 0.843, Accuracy: 0.620\n",
      "Epoch 14300, Loss: 0.839, Accuracy: 0.630\n",
      "Epoch 14400, Loss: 0.835, Accuracy: 0.633\n",
      "Epoch 14500, Loss: 0.832, Accuracy: 0.633\n",
      "Epoch 14600, Loss: 0.828, Accuracy: 0.623\n",
      "Epoch 14700, Loss: 0.824, Accuracy: 0.630\n",
      "Epoch 14800, Loss: 0.820, Accuracy: 0.637\n",
      "Epoch 14900, Loss: 0.817, Accuracy: 0.637\n",
      "Epoch 15000, Loss: 0.813, Accuracy: 0.637\n",
      "Epoch 15100, Loss: 0.810, Accuracy: 0.637\n",
      "Epoch 15200, Loss: 0.806, Accuracy: 0.650\n",
      "Epoch 15300, Loss: 0.803, Accuracy: 0.660\n",
      "Epoch 15400, Loss: 0.800, Accuracy: 0.667\n",
      "Epoch 15500, Loss: 0.797, Accuracy: 0.670\n",
      "Epoch 15600, Loss: 0.797, Accuracy: 0.660\n",
      "Epoch 15700, Loss: 0.805, Accuracy: 0.640\n",
      "Epoch 15800, Loss: 0.811, Accuracy: 0.633\n",
      "Epoch 15900, Loss: 0.815, Accuracy: 0.630\n",
      "Epoch 16000, Loss: 0.819, Accuracy: 0.623\n",
      "Epoch 16100, Loss: 0.821, Accuracy: 0.620\n",
      "Epoch 16200, Loss: 0.822, Accuracy: 0.630\n",
      "Epoch 16300, Loss: 0.823, Accuracy: 0.623\n",
      "Epoch 16400, Loss: 0.823, Accuracy: 0.627\n",
      "Epoch 16500, Loss: 0.823, Accuracy: 0.627\n",
      "Epoch 16600, Loss: 0.823, Accuracy: 0.630\n",
      "Epoch 16700, Loss: 0.822, Accuracy: 0.630\n",
      "Epoch 16800, Loss: 0.821, Accuracy: 0.630\n",
      "Epoch 16900, Loss: 0.820, Accuracy: 0.630\n",
      "Epoch 17000, Loss: 0.819, Accuracy: 0.630\n",
      "Epoch 17100, Loss: 0.818, Accuracy: 0.627\n",
      "Epoch 17200, Loss: 0.816, Accuracy: 0.627\n",
      "Epoch 17300, Loss: 0.815, Accuracy: 0.627\n",
      "Epoch 17400, Loss: 0.813, Accuracy: 0.623\n",
      "Epoch 17500, Loss: 0.812, Accuracy: 0.630\n",
      "Epoch 17600, Loss: 0.810, Accuracy: 0.637\n",
      "Epoch 17700, Loss: 0.809, Accuracy: 0.643\n",
      "Epoch 17800, Loss: 0.807, Accuracy: 0.640\n",
      "Epoch 17900, Loss: 0.805, Accuracy: 0.643\n",
      "Epoch 18000, Loss: 0.804, Accuracy: 0.643\n",
      "Epoch 18100, Loss: 0.803, Accuracy: 0.640\n",
      "Epoch 18200, Loss: 0.801, Accuracy: 0.647\n",
      "Epoch 18300, Loss: 0.799, Accuracy: 0.650\n",
      "Epoch 18400, Loss: 0.797, Accuracy: 0.650\n",
      "Epoch 18500, Loss: 0.794, Accuracy: 0.650\n",
      "Epoch 18600, Loss: 0.793, Accuracy: 0.650\n",
      "Epoch 18700, Loss: 0.791, Accuracy: 0.650\n",
      "Epoch 18800, Loss: 0.789, Accuracy: 0.657\n",
      "Epoch 18900, Loss: 0.787, Accuracy: 0.660\n",
      "Epoch 19000, Loss: 0.784, Accuracy: 0.663\n",
      "Epoch 19100, Loss: 0.782, Accuracy: 0.667\n",
      "Epoch 19200, Loss: 0.779, Accuracy: 0.663\n",
      "Epoch 19300, Loss: 0.777, Accuracy: 0.663\n",
      "Epoch 19400, Loss: 0.775, Accuracy: 0.663\n",
      "Epoch 19500, Loss: 0.774, Accuracy: 0.660\n",
      "Epoch 19600, Loss: 0.772, Accuracy: 0.660\n",
      "Epoch 19700, Loss: 0.770, Accuracy: 0.667\n",
      "Epoch 19800, Loss: 0.768, Accuracy: 0.663\n",
      "Epoch 19900, Loss: 0.765, Accuracy: 0.670\n",
      "Epoch 20000, Loss: 0.762, Accuracy: 0.677\n",
      "Epoch 20100, Loss: 0.760, Accuracy: 0.673\n",
      "Epoch 20200, Loss: 0.759, Accuracy: 0.670\n",
      "Epoch 20300, Loss: 0.756, Accuracy: 0.670\n",
      "Epoch 20400, Loss: 0.755, Accuracy: 0.663\n",
      "Epoch 20500, Loss: 0.751, Accuracy: 0.683\n",
      "Epoch 20600, Loss: 0.749, Accuracy: 0.687\n",
      "Epoch 20700, Loss: 0.748, Accuracy: 0.687\n",
      "Epoch 20800, Loss: 0.745, Accuracy: 0.693\n",
      "Epoch 20900, Loss: 0.743, Accuracy: 0.697\n",
      "Epoch 21000, Loss: 0.742, Accuracy: 0.700\n",
      "Epoch 21100, Loss: 0.740, Accuracy: 0.703\n",
      "Epoch 21200, Loss: 0.739, Accuracy: 0.707\n",
      "Epoch 21300, Loss: 0.737, Accuracy: 0.703\n",
      "Epoch 21400, Loss: 0.736, Accuracy: 0.707\n",
      "Epoch 21500, Loss: 0.734, Accuracy: 0.703\n",
      "Epoch 21600, Loss: 0.732, Accuracy: 0.707\n",
      "Epoch 21700, Loss: 0.730, Accuracy: 0.703\n",
      "Epoch 21800, Loss: 0.728, Accuracy: 0.707\n",
      "Epoch 21900, Loss: 0.725, Accuracy: 0.713\n",
      "Epoch 22000, Loss: 0.723, Accuracy: 0.723\n",
      "Epoch 22100, Loss: 0.721, Accuracy: 0.727\n",
      "Epoch 22200, Loss: 0.719, Accuracy: 0.727\n",
      "Epoch 22300, Loss: 0.717, Accuracy: 0.730\n",
      "Epoch 22400, Loss: 0.715, Accuracy: 0.730\n",
      "Epoch 22500, Loss: 0.711, Accuracy: 0.737\n",
      "Epoch 22600, Loss: 0.709, Accuracy: 0.737\n",
      "Epoch 22700, Loss: 0.707, Accuracy: 0.737\n",
      "Epoch 22800, Loss: 0.705, Accuracy: 0.737\n",
      "Epoch 22900, Loss: 0.704, Accuracy: 0.737\n",
      "Epoch 23000, Loss: 0.702, Accuracy: 0.740\n",
      "Epoch 23100, Loss: 0.700, Accuracy: 0.740\n",
      "Epoch 23200, Loss: 0.698, Accuracy: 0.743\n",
      "Epoch 23300, Loss: 0.696, Accuracy: 0.743\n",
      "Epoch 23400, Loss: 0.694, Accuracy: 0.743\n",
      "Epoch 23500, Loss: 0.691, Accuracy: 0.743\n",
      "Epoch 23600, Loss: 0.689, Accuracy: 0.743\n",
      "Epoch 23700, Loss: 0.687, Accuracy: 0.743\n",
      "Epoch 23800, Loss: 0.685, Accuracy: 0.743\n",
      "Epoch 23900, Loss: 0.682, Accuracy: 0.743\n",
      "Epoch 24000, Loss: 0.680, Accuracy: 0.743\n",
      "Epoch 24100, Loss: 0.677, Accuracy: 0.743\n",
      "Epoch 24200, Loss: 0.676, Accuracy: 0.743\n",
      "Epoch 24300, Loss: 0.677, Accuracy: 0.740\n",
      "Epoch 24400, Loss: 0.675, Accuracy: 0.740\n",
      "Epoch 24500, Loss: 0.673, Accuracy: 0.737\n",
      "Epoch 24600, Loss: 0.671, Accuracy: 0.737\n",
      "Epoch 24700, Loss: 0.672, Accuracy: 0.737\n",
      "Epoch 24800, Loss: 0.669, Accuracy: 0.737\n",
      "Epoch 24900, Loss: 0.668, Accuracy: 0.737\n",
      "Epoch 25000, Loss: 0.668, Accuracy: 0.733\n",
      "Epoch 25100, Loss: 0.668, Accuracy: 0.737\n",
      "Epoch 25200, Loss: 0.667, Accuracy: 0.737\n",
      "Epoch 25300, Loss: 0.666, Accuracy: 0.730\n",
      "Epoch 25400, Loss: 0.663, Accuracy: 0.733\n",
      "Epoch 25500, Loss: 0.660, Accuracy: 0.733\n",
      "Epoch 25600, Loss: 0.658, Accuracy: 0.733\n",
      "Epoch 25700, Loss: 0.656, Accuracy: 0.733\n",
      "Epoch 25800, Loss: 0.654, Accuracy: 0.733\n",
      "Epoch 25900, Loss: 0.653, Accuracy: 0.733\n",
      "Epoch 26000, Loss: 0.651, Accuracy: 0.733\n",
      "Epoch 26100, Loss: 0.647, Accuracy: 0.733\n",
      "Epoch 26200, Loss: 0.644, Accuracy: 0.737\n",
      "Epoch 26300, Loss: 0.641, Accuracy: 0.737\n",
      "Epoch 26400, Loss: 0.638, Accuracy: 0.737\n",
      "Epoch 26500, Loss: 0.636, Accuracy: 0.737\n",
      "Epoch 26600, Loss: 0.637, Accuracy: 0.737\n",
      "Epoch 26700, Loss: 0.634, Accuracy: 0.737\n",
      "Epoch 26800, Loss: 0.632, Accuracy: 0.740\n",
      "Epoch 26900, Loss: 0.631, Accuracy: 0.740\n",
      "Epoch 27000, Loss: 0.623, Accuracy: 0.750\n",
      "Epoch 27100, Loss: 0.618, Accuracy: 0.747\n",
      "Epoch 27200, Loss: 0.617, Accuracy: 0.747\n",
      "Epoch 27300, Loss: 0.632, Accuracy: 0.743\n",
      "Epoch 27400, Loss: 0.598, Accuracy: 0.723\n",
      "Epoch 27500, Loss: 0.634, Accuracy: 0.710\n",
      "Epoch 27600, Loss: 0.626, Accuracy: 0.710\n",
      "Epoch 27700, Loss: 0.621, Accuracy: 0.723\n",
      "Epoch 27800, Loss: 0.679, Accuracy: 0.660\n",
      "Epoch 27900, Loss: 0.656, Accuracy: 0.673\n",
      "Epoch 28000, Loss: 0.644, Accuracy: 0.670\n",
      "Epoch 28100, Loss: 0.635, Accuracy: 0.670\n",
      "Epoch 28200, Loss: 0.617, Accuracy: 0.683\n",
      "Epoch 28300, Loss: 0.622, Accuracy: 0.690\n",
      "Epoch 28400, Loss: 0.614, Accuracy: 0.700\n",
      "Epoch 28500, Loss: 0.637, Accuracy: 0.717\n",
      "Epoch 28600, Loss: 0.643, Accuracy: 0.727\n",
      "Epoch 28700, Loss: 0.624, Accuracy: 0.730\n",
      "Epoch 28800, Loss: 0.607, Accuracy: 0.737\n",
      "Epoch 28900, Loss: 0.650, Accuracy: 0.703\n",
      "Epoch 29000, Loss: 0.631, Accuracy: 0.710\n",
      "Epoch 29100, Loss: 0.607, Accuracy: 0.717\n",
      "Epoch 29200, Loss: 0.646, Accuracy: 0.710\n",
      "Epoch 29300, Loss: 0.622, Accuracy: 0.733\n",
      "Epoch 29400, Loss: 0.673, Accuracy: 0.690\n",
      "Epoch 29500, Loss: 0.642, Accuracy: 0.707\n",
      "Epoch 29600, Loss: 0.643, Accuracy: 0.710\n",
      "Epoch 29700, Loss: 0.627, Accuracy: 0.713\n",
      "Epoch 29800, Loss: 0.688, Accuracy: 0.657\n",
      "Epoch 29900, Loss: 0.663, Accuracy: 0.670\n",
      "Epoch 30000, Loss: 0.648, Accuracy: 0.677\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activate(inputs, activation_function):\n",
    "    if activation_function == \"relu\":\n",
    "        return np.maximum(0, inputs)\n",
    "    elif activation_function == \"softmax\":\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "    return inputs\n",
    "\n",
    "def d_activate(inputs, activation_function):\n",
    "    if activation_function == \"relu\":\n",
    "        return (inputs > 0).astype(float)\n",
    "    return np.ones_like(inputs)\n",
    "\n",
    "def cross_entropy_loss(y_hat, y):\n",
    "    samples = len(y_hat)\n",
    "    y_hat_clipped = np.clip(y_hat, 1e-7, 1 - 1e-7)\n",
    "    if len(y.shape) == 1:\n",
    "        correct_confidences = y_hat_clipped[range(samples), y]\n",
    "    else:\n",
    "        correct_confidences = np.sum(y_hat_clipped * y, axis=1)\n",
    "    return -np.mean(np.log(correct_confidences))\n",
    "\n",
    "def d_cross_entropy_loss(y_hat, y):\n",
    "    samples = len(y_hat)\n",
    "    if len(y.shape) == 1:\n",
    "        y = np.eye(y_hat.shape[1])[y]\n",
    "    return (y_hat - y) / samples\n",
    "\n",
    "def propagate_forward(weights, activations, biases, activation_function):\n",
    "    return activate(np.dot(activations, weights) + biases.T, activation_function)\n",
    "\n",
    "def propagate_backward(weights, activations, dl_dz, biases, activation_function):\n",
    "    d_activation = d_activate(np.dot(activations, weights) + biases.T, activation_function)\n",
    "    dl_dz *= d_activation\n",
    "    d_weights = np.dot(activations.T, dl_dz)\n",
    "    d_biases = np.sum(dl_dz, axis=0, keepdims=True).T\n",
    "    d_inputs = np.dot(dl_dz, weights.T)\n",
    "    return d_weights, d_biases, d_inputs\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self, input_dim, output_dim, neurons=[]):\n",
    "        self.weights = [0.01 * np.random.randn(n, m) for n, m in zip([input_dim] + neurons, neurons + [output_dim])]\n",
    "        self.biases = [0.01 * np.random.randn(n, 1) for n in neurons + [output_dim]]\n",
    "        self.activation_functions = [\"relu\"] * len(neurons) + [\"softmax\"]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.activations = [x]\n",
    "        for layer_weights, layer_biases, activation_function in zip(self.weights, self.biases, self.activation_functions):\n",
    "            x = propagate_forward(layer_weights, x, layer_biases, activation_function)\n",
    "            self.activations.append(x)\n",
    "        return x\n",
    "    \n",
    "    def adjust_weights(self, x, y, learning_rate=1e-4):\n",
    "        y_hat = self.forward(x)\n",
    "        loss_grad = d_cross_entropy_loss(y_hat, y)\n",
    "        dl_dz = loss_grad\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            d_weights, d_biases, dl_dz = propagate_backward(self.weights[i], self.activations[i], dl_dz, self.biases[i], self.activation_functions[i])\n",
    "            self.weights[i] -= learning_rate * d_weights\n",
    "            self.biases[i] -= learning_rate * d_biases\n",
    "    \n",
    "    def train_net(self, x, y, batch_size=32, epochs=100, learning_rate=1e-4):\n",
    "        for epoch in range(epochs + 1):\n",
    "            self.adjust_weights(x, y, learning_rate)\n",
    "            if epoch % 100 == 0:\n",
    "                y_hat = self.forward(x)\n",
    "                loss = cross_entropy_loss(y_hat, y)\n",
    "                predictions = np.argmax(y_hat, axis=1)\n",
    "                accuracy = np.mean(predictions == y)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.3f}, Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "def spiral_data(samples, classes):\n",
    "    X = np.zeros((samples * classes, 2))\n",
    "    y = np.zeros(samples * classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples * class_number, samples * (class_number + 1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number * 4, (class_number + 1) * 4, samples) + np.random.randn(samples) * 0.2\n",
    "        X[ix] = np.c_[r * np.sin(t * 2.5), r * np.cos(t * 2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y = spiral_data(100, 3)\n",
    "nn = NeuralNet(input_dim=2, output_dim=3, neurons=[64])\n",
    "nn.train_net(X, y, epochs=30000, learning_rate=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
