{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca31d406",
   "metadata": {},
   "source": [
    "# Machine Learning - Assignment 3\n",
    "\n",
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531893a",
   "metadata": {},
   "source": [
    "The aim of the assignment is to implement an artificial neural network (mostly) from scratch. This includes implementing or fixing the following:\n",
    "\n",
    "* Add support for additional activation functions and their derivatives.\n",
    "* Add support for loss functions and their derivative.\n",
    "* Add the use of a bias in the forward propagation.\n",
    "* Add the use of a bias in the backward propagation.\n",
    "\n",
    "In addition, you will we doing the following as well:\n",
    "\n",
    "* Test the algorithm on 3 datasets.\n",
    "* Compare neural networks with and without scaling.\n",
    "* Hyper-parameter tuning.\n",
    "\n",
    "The forward and backward propagation is made to work through a single layer, and are re-used multiple times to work for multiple layers.\n",
    "\n",
    "Follow the instructions and implement what is missing to complete the assignment. Some functions have been started to help you a little bit with the implementation.\n",
    "\n",
    "**Note:** You might need to go back and forth during your implementation of the code. The structure is set up to make implementation easier, you might find yourself going back and and forth to change something to make it easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb200d9",
   "metadata": {},
   "source": [
    "## Assignment preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a64bd",
   "metadata": {},
   "source": [
    "We help you out with importing the libraries.\n",
    "\n",
    "**IMPORTANT NOTE:** You may not import any more libraries than the ones already imported!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We set seed to better reproduce results later on.\n",
    "#np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1242d5e",
   "metadata": {},
   "source": [
    "## Neural Network utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d3e094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, accuracy: 0.360, loss: 1.099, lr: 0.05\n",
      "epoch: 100, accuracy: 0.717, loss: 0.675, lr: 0.04999752512250644\n",
      "epoch: 200, accuracy: 0.793, loss: 0.525, lr: 0.04999502549496326\n",
      "epoch: 300, accuracy: 0.857, loss: 0.431, lr: 0.049992526117345455\n",
      "epoch: 400, accuracy: 0.860, loss: 0.377, lr: 0.04999002698961558\n",
      "epoch: 500, accuracy: 0.900, loss: 0.321, lr: 0.049987528111736124\n",
      "epoch: 600, accuracy: 0.900, loss: 0.281, lr: 0.049985029483669646\n",
      "epoch: 700, accuracy: 0.913, loss: 0.249, lr: 0.049982531105378675\n",
      "epoch: 800, accuracy: 0.917, loss: 0.231, lr: 0.04998003297682575\n",
      "epoch: 900, accuracy: 0.917, loss: 0.211, lr: 0.049977535097973466\n",
      "epoch: 1000, accuracy: 0.920, loss: 0.195, lr: 0.049975037468784345\n",
      "epoch: 1100, accuracy: 0.927, loss: 0.195, lr: 0.049972540089220974\n",
      "epoch: 1200, accuracy: 0.933, loss: 0.186, lr: 0.04997004295924593\n",
      "epoch: 1300, accuracy: 0.933, loss: 0.180, lr: 0.04996754607882181\n",
      "epoch: 1400, accuracy: 0.933, loss: 0.176, lr: 0.049965049447911185\n",
      "epoch: 1500, accuracy: 0.927, loss: 0.171, lr: 0.04996255306647668\n",
      "epoch: 1600, accuracy: 0.927, loss: 0.168, lr: 0.049960056934480884\n",
      "epoch: 1700, accuracy: 0.930, loss: 0.164, lr: 0.04995756105188642\n",
      "epoch: 1800, accuracy: 0.933, loss: 0.160, lr: 0.049955065418655915\n",
      "epoch: 1900, accuracy: 0.933, loss: 0.157, lr: 0.04995257003475201\n",
      "epoch: 2000, accuracy: 0.933, loss: 0.153, lr: 0.04995007490013731\n",
      "epoch: 2100, accuracy: 0.943, loss: 0.150, lr: 0.0499475800147745\n",
      "epoch: 2200, accuracy: 0.947, loss: 0.146, lr: 0.0499450853786262\n",
      "epoch: 2300, accuracy: 0.950, loss: 0.143, lr: 0.0499425909916551\n",
      "epoch: 2400, accuracy: 0.950, loss: 0.140, lr: 0.04994009685382384\n",
      "epoch: 2500, accuracy: 0.953, loss: 0.138, lr: 0.04993760296509512\n",
      "epoch: 2600, accuracy: 0.933, loss: 0.157, lr: 0.049935109325431604\n",
      "epoch: 2700, accuracy: 0.953, loss: 0.134, lr: 0.049932615934796004\n",
      "epoch: 2800, accuracy: 0.953, loss: 0.132, lr: 0.04993012279315098\n",
      "epoch: 2900, accuracy: 0.953, loss: 0.130, lr: 0.049927629900459285\n",
      "epoch: 3000, accuracy: 0.957, loss: 0.129, lr: 0.049925137256683606\n",
      "epoch: 3100, accuracy: 0.957, loss: 0.127, lr: 0.04992264486178666\n",
      "epoch: 3200, accuracy: 0.957, loss: 0.125, lr: 0.04992015271573119\n",
      "epoch: 3300, accuracy: 0.957, loss: 0.124, lr: 0.04991766081847992\n",
      "epoch: 3400, accuracy: 0.953, loss: 0.122, lr: 0.049915169169995596\n",
      "epoch: 3500, accuracy: 0.953, loss: 0.120, lr: 0.049912677770240964\n",
      "epoch: 3600, accuracy: 0.960, loss: 0.119, lr: 0.049910186619178794\n",
      "epoch: 3700, accuracy: 0.763, loss: 0.913, lr: 0.04990769571677183\n",
      "epoch: 3800, accuracy: 0.957, loss: 0.119, lr: 0.04990520506298287\n",
      "epoch: 3900, accuracy: 0.960, loss: 0.117, lr: 0.04990271465777467\n",
      "epoch: 4000, accuracy: 0.960, loss: 0.116, lr: 0.049900224501110035\n",
      "epoch: 4100, accuracy: 0.963, loss: 0.115, lr: 0.04989773459295174\n",
      "epoch: 4200, accuracy: 0.963, loss: 0.114, lr: 0.04989524493326262\n",
      "epoch: 4300, accuracy: 0.963, loss: 0.113, lr: 0.04989275552200545\n",
      "epoch: 4400, accuracy: 0.963, loss: 0.112, lr: 0.04989026635914307\n",
      "epoch: 4500, accuracy: 0.960, loss: 0.111, lr: 0.04988777744463829\n",
      "epoch: 4600, accuracy: 0.963, loss: 0.110, lr: 0.049885288778453954\n",
      "epoch: 4700, accuracy: 0.960, loss: 0.109, lr: 0.049882800360552884\n",
      "epoch: 4800, accuracy: 0.963, loss: 0.108, lr: 0.04988031219089794\n",
      "epoch: 4900, accuracy: 0.963, loss: 0.107, lr: 0.049877824269451976\n",
      "epoch: 5000, accuracy: 0.960, loss: 0.106, lr: 0.04987533659617785\n",
      "epoch: 5100, accuracy: 0.960, loss: 0.105, lr: 0.04987284917103844\n",
      "epoch: 5200, accuracy: 0.960, loss: 0.105, lr: 0.04987036199399661\n",
      "epoch: 5300, accuracy: 0.960, loss: 0.105, lr: 0.04986787506501525\n",
      "epoch: 5400, accuracy: 0.960, loss: 0.103, lr: 0.04986538838405724\n",
      "epoch: 5500, accuracy: 0.960, loss: 0.102, lr: 0.049862901951085496\n",
      "epoch: 5600, accuracy: 0.957, loss: 0.102, lr: 0.049860415766062906\n",
      "epoch: 5700, accuracy: 0.960, loss: 0.101, lr: 0.0498579298289524\n",
      "epoch: 5800, accuracy: 0.957, loss: 0.100, lr: 0.04985544413971689\n",
      "epoch: 5900, accuracy: 0.957, loss: 0.099, lr: 0.049852958698319315\n",
      "epoch: 6000, accuracy: 0.957, loss: 0.098, lr: 0.04985047350472258\n",
      "epoch: 6100, accuracy: 0.957, loss: 0.097, lr: 0.04984798855888967\n",
      "epoch: 6200, accuracy: 0.957, loss: 0.097, lr: 0.049845503860783506\n",
      "epoch: 6300, accuracy: 0.960, loss: 0.096, lr: 0.049843019410367055\n",
      "epoch: 6400, accuracy: 0.957, loss: 0.095, lr: 0.04984053520760327\n",
      "epoch: 6500, accuracy: 0.963, loss: 0.095, lr: 0.049838051252455155\n",
      "epoch: 6600, accuracy: 0.963, loss: 0.094, lr: 0.049835567544885655\n",
      "epoch: 6700, accuracy: 0.947, loss: 0.117, lr: 0.04983308408485778\n",
      "epoch: 6800, accuracy: 0.953, loss: 0.096, lr: 0.0498306008723345\n",
      "epoch: 6900, accuracy: 0.957, loss: 0.094, lr: 0.04982811790727884\n",
      "epoch: 7000, accuracy: 0.957, loss: 0.094, lr: 0.04982563518965381\n",
      "epoch: 7100, accuracy: 0.957, loss: 0.093, lr: 0.049823152719422406\n",
      "epoch: 7200, accuracy: 0.960, loss: 0.092, lr: 0.049820670496547675\n",
      "epoch: 7300, accuracy: 0.960, loss: 0.092, lr: 0.04981818852099264\n",
      "epoch: 7400, accuracy: 0.960, loss: 0.091, lr: 0.049815706792720335\n",
      "epoch: 7500, accuracy: 0.963, loss: 0.091, lr: 0.0498132253116938\n",
      "epoch: 7600, accuracy: 0.963, loss: 0.090, lr: 0.04981074407787611\n",
      "epoch: 7700, accuracy: 0.963, loss: 0.090, lr: 0.049808263091230306\n",
      "epoch: 7800, accuracy: 0.963, loss: 0.089, lr: 0.04980578235171948\n",
      "epoch: 7900, accuracy: 0.963, loss: 0.089, lr: 0.04980330185930667\n",
      "epoch: 8000, accuracy: 0.963, loss: 0.088, lr: 0.04980082161395499\n",
      "epoch: 8100, accuracy: 0.967, loss: 0.100, lr: 0.04979834161562752\n",
      "epoch: 8200, accuracy: 0.963, loss: 0.089, lr: 0.04979586186428736\n",
      "epoch: 8300, accuracy: 0.960, loss: 0.094, lr: 0.04979338235989761\n",
      "epoch: 8400, accuracy: 0.967, loss: 0.087, lr: 0.04979090310242139\n",
      "epoch: 8500, accuracy: 0.963, loss: 0.087, lr: 0.049788424091821805\n",
      "epoch: 8600, accuracy: 0.963, loss: 0.086, lr: 0.049785945328062006\n",
      "epoch: 8700, accuracy: 0.967, loss: 0.086, lr: 0.0497834668111051\n",
      "epoch: 8800, accuracy: 0.967, loss: 0.086, lr: 0.049780988540914256\n",
      "epoch: 8900, accuracy: 0.970, loss: 0.085, lr: 0.0497785105174526\n",
      "epoch: 9000, accuracy: 0.970, loss: 0.085, lr: 0.04977603274068329\n",
      "epoch: 9100, accuracy: 0.970, loss: 0.084, lr: 0.04977355521056952\n",
      "epoch: 9200, accuracy: 0.970, loss: 0.084, lr: 0.049771077927074414\n",
      "epoch: 9300, accuracy: 0.970, loss: 0.083, lr: 0.0497686008901612\n",
      "epoch: 9400, accuracy: 0.970, loss: 0.083, lr: 0.04976612409979302\n",
      "epoch: 9500, accuracy: 0.970, loss: 0.082, lr: 0.0497636475559331\n",
      "epoch: 9600, accuracy: 0.970, loss: 0.082, lr: 0.049761171258544616\n",
      "epoch: 9700, accuracy: 0.970, loss: 0.082, lr: 0.0497586952075908\n",
      "epoch: 9800, accuracy: 0.970, loss: 0.086, lr: 0.04975621940303483\n",
      "epoch: 9900, accuracy: 0.970, loss: 0.083, lr: 0.049753743844839965\n",
      "epoch: 10000, accuracy: 0.970, loss: 0.084, lr: 0.04975126853296942\n",
      "validation, acc: 0.810, loss: 0.774\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def spiral_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples) # radius\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # We need to know the size of the input thats coming in, and how many neurons we want to have (in each layer?)\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        # Why use an activation function? \n",
    "        # We use it to create more complex decisions IE, we can then fit non linear data. We need non linear actvation function to fit non linear data\n",
    "        # activation = \n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "class Activation_ReLu:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs # Remember input values\n",
    "        self.output = np.maximum(0, inputs) # Calculate output values from inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0  # Zero gradient where input values were negative\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs  # Remember input values\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True)) # Get non normalized probs + Overflow prevention: v = u - max u\n",
    "        probapilities = exp_values / np.sum(exp_values, axis=1, keepdims=True) # Normalize values\n",
    "        self.output = probapilities \n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues) # Skapar en tom array med samma shape som dvalues\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1, 1) # Flatten output array\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses) # or batch loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true): # y_pred NN predictions;  y_true target training values\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) # To prevent dividing by 0\n",
    "\n",
    "        if len(y_true.shape) == 1: # Then we have passed scalar values\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples), \n",
    "                y_true\n",
    "            ]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues) # Number of samples\n",
    "        labels = len(dvalues[0]) # Number of labels in every sample\n",
    "        \n",
    "        # If labels are spare, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        #Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded we turn them into discrete values\n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        self.dinputs = dvalues.copy() # Copy to safely modify \n",
    "        self.dinputs[range(samples), y_true] -= 1 # Calculate gradient \n",
    "        self.dinputs = self.dinputs / samples # Normalize gradient\n",
    "\n",
    "# Optimizer Stochastic Gradient Descent \n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "    \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum: \n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                            \n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates   = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "            (1. / (1. + self.decay * self.iterations))\n",
    "        \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / \\\n",
    "                        (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / \\\n",
    "                        (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "num_classes = 3\n",
    "\n",
    "n_inputs = len(X[0])\n",
    "\n",
    "dense1 = Layer_Dense(n_inputs, 64)\n",
    "activation1 = Activation_ReLu()\n",
    "dense2 = Layer_Dense(64, num_classes)\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()\n",
    "\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
    "\n",
    "#optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
    "\n",
    "for epoch in range(10001):\n",
    "\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, \" +  \n",
    "            f\"accuracy: {accuracy:.3f}, \" + \n",
    "            f\"loss: {loss:.3f}, \" +\n",
    "            f\"lr: {optimizer.current_learning_rate}\"\n",
    "        )\n",
    "\n",
    "    # Backward pass (Back propagation)\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0c6ac",
   "metadata": {},
   "source": [
    "### 1) Activation functions\n",
    "\n",
    "Below is some setup for choosing activation function. Implement 2 additional activation functions, \"ReLU\" and one more of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activations = input signal\n",
    "# Activation functions\n",
    "def activate(activations, selected_function = \"none\"):\n",
    "    y = activations\n",
    "    if selected_function == \"none\":\n",
    "        y = activations\n",
    "    elif selected_function == \"relu\":\n",
    "        y = np.maximum(0, activations)\n",
    "    elif selected_function == \"elu\":\n",
    "        alpha = 1\n",
    "        y = np.where(activations > 0, activations, alpha * (np.exp(activations) - 1))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ebbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Test your activation functions, is the returning values what you expect?\n",
    "print(activate(0, selected_function=\"elu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7f8fa",
   "metadata": {},
   "source": [
    "### 2) Activation function derivatives\n",
    "\n",
    "Neural networks need both the activation function and its derivative. Finish the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c00420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU: https://stats.stackexchange.com/questions/333394/what-is-the-derivative-of-the-relu-activation-function\n",
    "# ELU:  https://medium.com/@krishnakalyan3/introduction-to-exponential-linear-unit-d3e2904b366c\n",
    "def d_activate(activations, selected_function = \"none\"):\n",
    "    dy = 0\n",
    "    if selected_function == \"none\":\n",
    "        dy = np.ones_like(activations)\n",
    "    elif selected_function == \"relu\":\n",
    "        dy =  np.where(activations > 0, 1, 0)\n",
    "    elif selected_function == \"elu\" :\n",
    "        alpha = 1\n",
    "        dy = np.where(activations > 0, 1, alpha * np.exp(activations))\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea074e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Test your activation function derivatives, is the returning values what you expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f72d0",
   "metadata": {},
   "source": [
    "### 3) Loss functions\n",
    "\n",
    "To penalize the network when it predicts incorrect, we need to meassure how \"bad\" the prediction is. This is done with loss-functions.\n",
    "\n",
    "Similar as with the activation functions, the loss function needs its derivative as well.\n",
    "\n",
    "Finish the MSE_loss (Mean Squared Error loss), as well as adding one additional loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the loss for a set of predictions y_hat compared to a set of real valyes y\n",
    "def MSE_loss(y_hat, y): # y_hat = predictions, y = the real values (targets)\n",
    "    y_hat = np.array(y_hat)\n",
    "    y = np.array(y)\n",
    "    loss = np.mean(np.square(np.subtract(y, y_hat)))\n",
    "    return loss\n",
    "\n",
    "y_h = [2,3,4,5]\n",
    "y = [1,1,1,1]\n",
    "\n",
    "print(MSE_loss(y_h,y))\n",
    "\n",
    "# TODO: Choose another loss function and implement it\n",
    "def MAE_loss(y_hat, y):\n",
    "    y_hat = np.array(y_hat)\n",
    "    y = np.array(y)\n",
    "    loss = np.mean(np.abs(np.subtract(y, y_hat)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965be68",
   "metadata": {},
   "source": [
    "The derivatives of the loss is with respect to the predicted value **y_hat**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea0d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_MSE_loss(y_hat, y): # y_hat = predictions, y = the real values (targets)\n",
    "    y_hat = np.array(y_hat)\n",
    "    y = np.array(y)\n",
    "    dy = (2 / len(y_hat)) * np.subtract(y, y_hat)\n",
    "    return dy\n",
    "\n",
    "# TODO: Choose another loss function and implement it\n",
    "def d_MAE_loss(y_hat, y):\n",
    "    y_hat = np.array(y_hat)\n",
    "    y = np.array(y)\n",
    "    dy = (2 / len(y_hat)) * np.subtract(y_hat, y)\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfeb3b",
   "metadata": {},
   "source": [
    "### 4) Forward propagation\n",
    "\n",
    "The first \"fundamental\" function for neural networks is to be able to propagate the data forward through the neural network. We will implement this function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_forward(weights, activations, biases, activation_function=\"none\"):\n",
    "    # NOTE: activations = input \n",
    "    dot_product = np.dot(activations, weights) + biases\n",
    "    new_activations = activate(dot_product, activation_function)\n",
    "\n",
    "    return new_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4abe8c",
   "metadata": {},
   "source": [
    "### 5) Back-propagation\n",
    "\n",
    "To be able to train a neural network, we need to be able to propagate the loss backwards and update the weights. We will implement this function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the backward gradients that are passed throught the layer in the backward pass.\n",
    "# Returns both the derivative of the loss in respect to the weights and the input signal (activations).\n",
    "\n",
    "def propagate_backward(weights, activations, dl_dz, biases, activation_function=\"none\"):\n",
    "    # NOTE: dl_dz is the derivative of the loss based on the previous layers activations/outputs\n",
    "    \n",
    "    dot_product = np.dot(activations, weights) + biases  # Transpose bias to match the shape of dot_product\n",
    "    d_loss = d_activate(dot_product, activation_function) * dl_dz\n",
    "    d_weights = np.dot(activations.T, d_loss)\n",
    "    d_activations = np.dot(d_loss, weights.T)\n",
    "    d_biases = np.sum(d_loss, axis=0, keepdims=True).T  # Transpose to match the shape of biases\n",
    "    return d_weights, d_biases, d_activations\n",
    "\n",
    "# Test\n",
    "# Example usage\n",
    "weights = np.array([[0.2, 0.8], [0.5, 0.1]])\n",
    "activations = np.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "dl_dz = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "bias = np.array([0.1, 0.2])\n",
    "\n",
    "d_weights, d_activations, d_loss = propagate_backward(weights, activations, dl_dz, bias, activation_function=\"relu\")\n",
    "print(\"d_weights:\", d_weights)\n",
    "print(\"d_activations:\", d_activations)\n",
    "print(\"d_loss:\", d_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73645779",
   "metadata": {},
   "source": [
    "## Neural network implementation\n",
    "\n",
    "### 6) Fixing the neural network\n",
    "\n",
    "Below is a class implementation of a MLP neural network. This implementation is still lacking several areas that are needed for the network to be robust and function well. Your task is to improve and fix it with the following:\n",
    "\n",
    "1. Add a bias to the activation functions, and make sure the bias is also updated during training. \n",
    "2. Add a function that trains the network using minibatches (such that the neural network trains on a few samples at a time). \n",
    "3. Make use of an validation set in the training function. The model should stop training when the loss starts to increase for the validatin set. This feature should be able to be turned on and off to test the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP = Multi Layer Perceptron Neural Network\n",
    "class NeuralNet(object):\n",
    "    \n",
    "    # Setup all parameters and activation functions.\n",
    "    # This function runs directly when a new instance of this class is created. \n",
    "    # Input_dim is the size of the input (number of features?), output_dim is the size of the output (number of classes) and neurons is a list of the number of neurons in each layer.\n",
    "    def __init__ (self, input_dim, output_dim, neurons = []):\n",
    "        # NOTE: The \"neurons\" parameter is given as a list.\n",
    "        # E.g., [4, 8, 4] means 4 neurons in layer 1, 8 neurons in layer 2 etc...\n",
    "\n",
    "        # TODO: Add support for bias for each neuron in the code below.\n",
    "        self.weights = [0.01 * np.random.randn(n, m) for n, m in zip([input_dim] + neurons, neurons + [output_dim])]\n",
    "        self.biases = [0.01 * np.random.randn(n, 1) for n in neurons + [output_dim]]\n",
    "        self.activation_functions = [\"relu\"] * len(neurons) + [\"none\"]\n",
    "    \n",
    "    # Predict the input throught the network and calculate the output.\n",
    "    def forward(self, x):\n",
    "        \"\"\" x is the input to the network, or to the next layer. \"\"\"\n",
    "        # TODO: Add support for a bias for each neuron in the code below.\n",
    "        for layer_weights, layer_biases, layer_activation_function in zip(self.weights, self.biases, self.activation_functions):\n",
    "            x = propagate_forward(layer_weights, x, layer_biases, layer_activation_function)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    # Adjust the weights in the network to better fit the desired output (y), given the input (x).\n",
    "    # The weight updates are happening \"in-place\", thus we are only returning the loss from this function.\n",
    "    # Note that this function can handle a variable size of the input (x), both full datasets or smaller parts of the dataset.\n",
    "    def adjust_weights(self, x, y, learning_rate=1e-4):\n",
    "        activation = x\n",
    "        activation_history = []\n",
    "        \n",
    "        for layer_weights, layer_biases, layer_activation_function in zip(self.weights, self.biases, self.activation_functions):\n",
    "            activation_history.append(activation)\n",
    "            activation = propagate_forward(layer_weights, activation, layer_biases, layer_activation_function)\n",
    "\n",
    "        loss = MSE_loss(activation, y)\n",
    "        d_activations = d_MSE_loss(activation, y)\n",
    "        \n",
    "        for layer_weights, layer_activation_function, layer_biases, previous_activations in reversed(list(zip(self.weights, self.activation_functions, self.biases, activation_history))):\n",
    "            d_weights, d_biases, d_activations = propagate_backward(layer_weights, previous_activations, d_activations, layer_biases, layer_activation_function)\n",
    "            layer_weights -= learning_rate * d_weights\n",
    "            layer_biases -= learning_rate * d_biases\n",
    "                    \n",
    "        return loss\n",
    "    \n",
    "    # A function for the training of the network.\n",
    "    def train_net(self, x, y, batch_size=32, epochs=100, learning_rate=1e-4, use_validation_data=False):\n",
    "        \n",
    "        # TODO: Add a training loop where the weights and biases of the network is learnt over several epochs.\n",
    "\n",
    "        # TODO: Add support for mini batches. That is, in each epoch the data should be split into several\n",
    "        #       smaller subsets and the model should be trained on each of these subsets one at a time.\n",
    "\n",
    "        # TODO: Implement the use of validation data, that is, splitting the training data into training data and validation data.\n",
    "        #       The validation data should be used to stop the training when the model stops to generalise and starts to overfit.\n",
    "        #       This feature should be able to be turned on and off to test the difference.\n",
    "\n",
    "        # NOTE: Make use of previously implemented functions here.\n",
    "\n",
    "        ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0cc71",
   "metadata": {},
   "source": [
    "## Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb678ea9",
   "metadata": {},
   "source": [
    "### 7) Simple test\n",
    "\n",
    "In this a very simple test for you to use and toy around with before using the datasets.\n",
    "\n",
    "Make sure to test both the **adjust_weights** function and the **train_net** function. What is the difference between the two?\n",
    "\n",
    "Also, be sure to **plot the loss for each epoch** to see how the network training is progressing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "input_dimension = 4\n",
    "output_dim = 1\n",
    "neurons = [18, 12]  # NOTE: 18 neurons in layer 1, 12 neurons in layer 2 etc...\n",
    "\n",
    "k = np.random.randint(0, 10, (input_dimension, 1))\n",
    "x = np.random.normal(0, 1, (n, input_dimension))\n",
    "y = np.dot(x, k) + 0.1 + np.random.normal(0, 0.01, (n, 1))\n",
    "\n",
    "# Create an instance of the NeuralNet class\n",
    "nn = NeuralNet(input_dimension, output_dim, neurons)\n",
    "\n",
    "# Adjust weights using the adjust_weights function\n",
    "loss_1 = [nn.adjust_weights(x, y) for _ in range(1000)]\n",
    "\n",
    "# Train the network using the train_net function\n",
    "loss_2 = []\n",
    "for epoch in range(100):\n",
    "    loss = nn.train_net(x, y, batch_size=32, epochs=1, learning_rate=1e-4, use_validation_data=False)\n",
    "    loss_2.append(loss)\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(loss_1)\n",
    "plt.title(\"Loss 1\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_2)\n",
    "plt.title(\"Loss 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d17c1",
   "metadata": {},
   "source": [
    "### Real test and preprocessing\n",
    "\n",
    "When using real data and neural networks, it is very important to scale the data between smaller values, usually between 0 and 1. This is because neural networks struggle with larger values as input compared to smaller values. \n",
    "\n",
    "To test this, we will use our first dataset and test with and without scaling.\n",
    "\n",
    "Similar as with assignment 2, we will use the scikit-learn library for this preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddc1c7",
   "metadata": {},
   "source": [
    "### 8) Dataset 1: Wine - with and without scaling\n",
    "\n",
    "Wine dataset: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Train two neural network, one with scaling and one without. Are we able to see any difference in training results or loss over time?\n",
    "\n",
    "**Note:** Do not train for to many epochs (more than maybe 50-100). The network might \"learn\" anyway in the end, but you should still be able to see a difference when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "data_wine = pd.read_csv(\"wine.csv\").to_numpy()\n",
    "\n",
    "# TODO: Set up the data and split it into train and test-sets.\n",
    "\n",
    "# TODO: Train and test your neural networks.\n",
    "# NOTE: Use the same train/test split for both neural network models!\n",
    "\n",
    "# TODO: Do the above at least 3 times\n",
    "# NOTE: Use loops here!\n",
    "\n",
    "# TODO: Plot the results with matplotlib (plt)\n",
    "# NOTE: One combined lineplot with the scaling and one without the scaling, 2 plots in total.\n",
    "# NOTE: Plot both the accuracy and the loss!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39ab63",
   "metadata": {},
   "source": [
    "### Real data and hyper-parameter tuning\n",
    "\n",
    "Now we are going to use real data, preprocess it, and do hyper-parameter tuning.\n",
    "\n",
    "Choose two hyper-parameters to tune to try and achive an even better result.\n",
    "\n",
    "**NOTE:** Changing the number of epochs should be part of the tuning, but it does not count towards the two hyper parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a7c7a",
   "metadata": {},
   "source": [
    "### 9) Dataset 2: Mushroom\n",
    "\n",
    "Mushroom dataset: https://archive.ics.uci.edu/dataset/73/mushroom\n",
    "\n",
    "Note: This dataset has one feature with missing values. Remove this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mushroom = pd.read_csv(\"mushroom.csv\").to_numpy()\n",
    "\n",
    "# TODO: Preprocess the data.\n",
    "\n",
    "# TODO: Split the data into train and test\n",
    "\n",
    "# TODO: Train a neural network on the data\n",
    "\n",
    "# TODO: Visualize the loss for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f363a",
   "metadata": {},
   "source": [
    "When hyper-parameter tuning, please write the parameters and network sizes you test here:\n",
    "\n",
    "* Parameter 1: \n",
    "* Parameter 2:\n",
    "\n",
    "* Neural network sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyper-parameter tuning\n",
    "\n",
    "# TODO: Visualize the loss after hyper-parameter tuning for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy after hyper-parameter tuning for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8991d79",
   "metadata": {},
   "source": [
    "### 10) Dataset 3: Adult\n",
    "\n",
    "Adult dataset: https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "**IMPORTANT NOTE:** This dataset is much larger than the previous two (48843 instances). If your code runs slow on your own computer, you may exclude parts of this dataset, but you must keep a minimum of 10000 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80862998",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = pd.read(...) # TODO: Read the data.\n",
    "\n",
    "# TODO: Preprocess the data.\n",
    "\n",
    "# TODO: Split the data into train and test\n",
    "\n",
    "# TODO: Train a neural network on the data\n",
    "\n",
    "# TODO: Visualize the loss for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9bd1ff",
   "metadata": {},
   "source": [
    "When hyper-parameter tuning, please write the parameters and network sizes you test here:\n",
    "\n",
    "* Parameter 1: \n",
    "* Parameter 2:\n",
    "\n",
    "* Neural network sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5229f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyper-parameter tuning\n",
    "\n",
    "# TODO: Visualize the loss after hyper-parameter tuning for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy after hyper-parameter tuning for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df892ad",
   "metadata": {},
   "source": [
    "# Questions for examination:\n",
    "\n",
    "In addition to completing the assignment with all its tasks, you should also prepare to answer the following questions:\n",
    "\n",
    "1) Why would we want to use different activation functions?\n",
    "\n",
    "2) Why would we want to use different loss functions?\n",
    "\n",
    "3) Why are neural networks sensitive to large input values?\n",
    "\n",
    "4) What is the role of the bias? \n",
    "\n",
    "5) What is the purpose of hyper-parameter tuning?\n",
    "\n",
    "6) A small example neural network will be shown during the oral examination. You will be asked a few basic questions related to the number of weights, biases, inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28b89a",
   "metadata": {},
   "source": [
    "If we don't apply an activation function, every node will just be a linear combination of every node before it (+ a bias term)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cf459",
   "metadata": {},
   "source": [
    "# Finished!\n",
    "\n",
    "Was part of the setup incorrect? Did you spot any inconsistencies in the assignment? Could something improve?\n",
    "\n",
    "If so, please write them and send via email and send it to:\n",
    "\n",
    "* marcus.gullstrand@ju.se\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
